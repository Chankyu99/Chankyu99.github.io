---
title: "데이터 전처리 기법, Z-Score에 대해 알아보기"
description: "표준화(Standardization)의 개념부터 장단점, 그리고 올바른 사용 시나리오까지"
date: 2025-11-27
categories: [Theory, Data Preprocessing]
tags: [Z-Score, Normalization, Standardization, Statistics, Feature Scaling]
math: true
---

[SQL 프로젝트](https://chankyu99.github.io/posts/Node_SQL_Project/)를 통해 데이터의 이상치(Outlier)를 제거하기 위해 **Z-Score** 기법을 사용했다.

하지만 무조건 Z-Score를 적용한다고 해서 모델 성능이 좋아지는 것은 아니다. 데이터의 분포나 특성에 따라 오히려 독이 될 수도 있다.

오늘은 Z-Score가 정확히 무엇인지, **어떤 장점이 있고 어떤 치명적인 단점이 있는지** 확실하게 정리해 보려고 한다.

---

# Z-Score(표준화)란?

**Z-Score 표준화(Standardization)**는 서로 다른 범위(Scale)를 가진 데이터를 **평균이 0, 표준편차가 1**인 분포로 변환하는 통계적 기법입니다.

$$
z = \frac{x - \mu}{\sigma}
$$

* $x$ : 원본 데이터 값
* $\mu$ : 데이터의 평균
* $\sigma$ : 데이터의 표준편차

쉽게 말해, **"어떤 데이터가 평균으로부터 표준편차의 몇 배만큼 떨어져 있는가?"**를 나타내는 수치입니다.

---

# Z-Score의 장점

### 1. 서로 다른 데이터의 비교 가능 (Standardization Effect)
단위가 다른 데이터(예: '키(cm)'와 '몸무게(kg)')를 하나의 척도로 통합하여 비교할 수 있다. 이는 다양한 출처의 데이터를 융합할 때 필수적이다.

### 2. 머신러닝 모델 성능 향상
데이터의 스케일이 들쑥날쑥하면 모델이 학습하는 데 애를 먹는다.
* **수렴 속도 가속화:** 기울기 기반 알고리즘(Gradient Descent)을 사용하는 모델(로지스틱 회귀, 신경망 등)의 학습 속도를 높여준다.
* **최적의 성능 유도:** SVM이나 K-Means처럼 '거리'를 기반으로 하는 알고리즘에서 특정 특성이 결과를 지배하는 것을 방지한다.

### 3. 효과적인 이상치 탐지 (Outlier Detection)
데이터가 정규분포를 따른다고 가정할 때, Z-Score는 이상치를 찾아내는 데 효과적이다.
* 일반적으로 **$\text{Z-score} > 3$** (평균에서 표준편차 3배 이상 벗어남)인 경우를 잠재적 이상치로 간주한다.

### 4. 특성 가중치 학습 보정
값의 범위가 큰 특성(예: 연봉)이 범위가 작은 특성(예: 나이)보다 모델에 과도한 영향을 미치는 것을 방지해 모든 특성이 **균등한 기회**를 갖도록 한다.
---

# Z-Score의 단점

### 1. '정규분포'라는 가정의 함정
Z-Score는 데이터가 **정규분포(Gaussian Distribution)**를 따른다는 가정하에 설계되었습니다.
데이터가 한쪽으로 심하게 치우쳐 있거나(Skewed), 비정규 분포를 따른다면 Z-Score의 결과는 신뢰하기 어렵습니다.

### 2. 극단적 이상치에 취약함
아이러니하게도 이상치를 찾기 위해 쓰지만, **극단적인 이상치(Extreme Outliers)**가 이미 포함되어 있다면 계산 자체가 망가집니다.
* 이유: 극단적인 값이 **평균($\mu$)과 표준편차($\sigma$)를 왜곡**시키기 때문에, 정상 데이터들의 Z-Score까지 엉뚱하게 계산될 수 있습니다.

### 3. 해석의 어려움 (Interpretability)
데이터를 변환하고 나면 원래의 단위(cm, kg, 원 등)가 사라집니다.
* "-1.5"라는 수치만 보고는 이것이 실제 비즈니스 환경에서 어떤 의미인지 직관적으로 해석하기 어렵습니다.

### 4. 정보의 손실
* **비율 정보 파괴:** 원본 데이터에서 2배 차이가 나던 값들이 표준화 후에는 그 비율이 유지되지 않습니다.
* **절대적 기준 상실:** 설문조사의 "동의함(5점)"과 같은 절대적인 의미가 사라지고 상대적인 위치만 남게 됩니다.

---

# 결론: 언제 사용해야 할까?

### 이런 경우
1.  데이터가 **정규분포(Bell Curve)**에 가깝게 생겼을 때 (예: 키, 혈압 등 자연 현상 데이터)
2.  **SVM, 로지스틱 회귀, 신경망** 등 기울기나 거리를 기반으로 하는 알고리즘을 쓸 때
3.  **PCA(주성분 분석)** 등 차원 축소 기법을 적용하기 전
4.  데이터에 극단적인 이상치가 별로 없을 때

### 대안
데이터가 정규분포가 아니거나 이상치가 너무 많다면?
* **Min-Max Scaling:** 데이터를 0과 1 사이로 압축
* **Robust Scaling:** 중앙값(Median)과 IQR을 사용하여 이상치에 영향을 덜 받게 스케일링
* **Log Transformation:** 데이터가 한쪽으로 치우쳤을 때 로그를 씌워 정규분포처럼 변환

--- 
[참고자료]
[1](https://aicompetence.org/z-scores-the-key-to-uncovering-data-anomalies/)
[2](https://developers.google.com/machine-learning/crash-course/numerical-data/normalization)
[3](https://www.aionlinecourse.com/ai-basics/z-score-standardization)
[4](https://www.geeksforgeeks.org/data-analysis/z-score-normalization-definition-and-examples/)
[5](https://www.datacamp.com/tutorial/normalization-vs-standardization)
[6](https://www.sciencedirect.com/science/article/pii/S2210670724003962)
[7](https://pmc.ncbi.nlm.nih.gov/articles/PMC12239870/)
[8](https://www.datacamp.com/tutorial/z-score)
[9](https://www.almabetter.com/bytes/tutorials/data-science/normalization-in-machine-learning)
[10](https://builtin.com/data-science/when-and-why-standardize-your-data)