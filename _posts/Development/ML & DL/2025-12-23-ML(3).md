---
title : "ML (3) - 앙상블(Ensemble)"
description : "앙상블(Ensemble)에 대해 알아보자."
date : 2025-12-23
categories : [Development, ML&DL]
tags : [Scikit-learn, Ensemble]
pin : false
math : true
mermaid : true
---

# 🌲 의사결정나무(Decision Tree)

의사결정나무(혹은 결정 트리라고 많이 부른다)가 실습 과정에서 복잡한 패턴을 가진 데이터에 트리 기반 모델이 강력한 성능을 보여주었다. 

그렇다면 트리 기반 모델이 강력한 성능을 보인 이유가 무엇인지 알고리즘의 기본 원리를 알아보자.

## ⚙️ 작동 원리

결정트리는 기본적으로 시작 노드(루트)에서 시작하여 데이터를 특정한 기준에 따라 분리해 아래로 뻗어나가는 노드(리프)를 생성한다. 

이 과정이 나무가 가지를 뻗어나가며 수많은 잎을 구성하는 것처럼 보여 '트리'라는 이름이 붙게 되었다.

![1](/assets/img/dt.png)
_약간 오래된 나무가 잎이 무성한 것처럼 보인다._

그렇다면 리프 노드를 구성할 때 데이터를 분리하는 기준이 무엇일까?

기본적으로 결정트리는 **<mark>정보이득(Information Gain)</mark>**이 최대가 되는 기준을 사용한다.

$$ \text{정보이득} = \text{부모 노드의 불순도} - (\text{왼쪽 자식 노드 불순도} \times \text{왼쪽 자식 노드 데이터 비율} - \text{오른쪽 자식 노드 불순도} \times \text{오른쪽 자식 노드 데이터 비율}) $$

정보이득이 최대가 되려면 **<mark>불순도(impurity)</mark>**가 최소이면 된다. 

결정트리 구성을 간단하게 확인해보면 다음과 같다.

![2](/assets/img/dt2.png)
_사이킷런 'Wine'데이터를 학습한 결정트리 노드의 일부_

네모모양의 노드안에 글씨를 설명하자면,

- `sugar` : 학습에 특성으로 사용한 `sugar`값의 대소비교를 통해 리프 노드가 나뉘어짐 
- `gini` : 지니 불순도
- `samples` : 해당 노드에 속한 샘플의 수 
- `value` : 해당 노드의 클래스별 샘플의 수 

## 🏷️ 노드를 분할하는 기준 : 불순도(impurity)

불순도는 결정 트리의 하이퍼파라미터 `criterion`을 통해 2가지 기준을 사용할 수 있다.

먼저, **<mark>지니 불순도(Gini impurity)</mark>**이다. 

$$ \text{지니 불순도} = 1 - (\text{음성 클래스 비율}^2 + \text{양성 클래스 비율}^2) $$

지니 불순도의 경우 두 클래스의 비율이 **정확히 반반**일경우 0.5, 한쪽 클래스의 비율이 **100%**일경우 0을 가지게 된다.

즉, 데이터 균일도가 **높을수록**(클래스 비율 불균형이 **적을수록**) 지니 불순도가 **낮아지게** 되고, 정보이득이 **최대**가 된다.

다음은 **<mark>엔트로피(Entropy)</mark>**이다.

$$ \text{엔트로피} = -\text{음성 클래스 비율} \log_2 \text{음성 클래스 비율} - \text{양성 클래스 비율} \log_2 \text{양성 클래스 비율} $$

지니 불순도와 같이 클래스 비율을 사용하며, 밑이 2인 로그를 사용해 곱한다는 특징이 있다.

두가지 불순도의 결과값은 차이가 매우 작다. 따라서 기본값인 지니 불순도를 계속 사용해도 무방하다.

## 🪾 하이퍼파라미터로 가지치기

나무가 올바르게 자라기위해 가지치기를 하는 것처럼, 결정 트리의 가지치기도 필요하다.

가지치기가 이루어지지 않는다면, 결정트리는 무한히 리프 노드를 생성해 지나친 학습으로 과적합의 우려가 있다.

때문에 우리는 하이퍼파라미터 `max_depth`를 사용하여 결정트리의 깊이를 제한함으로써 가지치기를 해주어야 한다.

`max_depth`로 조정한 깊이만큼 리프 노드를 생성하고, 마지막 리프 노드에서 가장 많은 클래스가 **예측 클래스**가 되는 것이다.

`min_impurity_decrease`는 결정트리가 정보이득을 통해 노드를 분할하는 과정에서 정보이득이 최소한 얼마 이상이어야 분할을 수행할 것인지를 결정하는 파라미터이다.

$$ \text{min_impurity_decrease} < \text{정보이득} \times \text{노드 샘플 비율} $$

수식이 성립해야만 분할을 수행하는 것이므로, `min_impurity_decrease`를 통해서도 결정트리의 가지치기가 가능하다.

## 🌟 의사결정나무의 장점

의사결정나무는 직관적인 구조 덕분에 시각화가 가능하여 비전문가도 쉽게 이해하고 해석할 수 있다는 강력한 장점이 있다. 

또한, 정보 이득 계산 시 클래스 비율을 사용하기 때문에 스케일링이나 정규화 같은 번거로운 전처리 과정이 필요 없으며, 결측치가 있거나 수치형 및 범주형 데이터가 섞여 있어도 유연하게 처리할 수 있다. 

데이터 생성 모델의 가정이 다소 위배되더라도 비교적 안정적인 성능을 보여주는 강건함(Robustness) 또한 큰 매력이다.

## ⚠️ 의사결정나무의 단점

반면, 적절한 가지치기(Pruning)가 없다면 훈련 데이터에 너무 깊게 학습되어 과적합(Overfitting)되기 쉽고, 데이터의 아주 작은 변화에도 트리 구조 전체가 바뀔 수 있는 불안정성을 가진다. 

또한 훈련 데이터의 범위를 벗어난 값을 예측하는 외삽(Extrapolation) 능력이 부족하고, XOR 같은 복잡한 관계를 표현하는 데 한계가 있으며, 탐욕적(Greedy) 알고리즘을 사용하기 때문에 전역 최적해를 보장하기 어렵다는 점도 고려해야 한다. 

특히 데이터 불균형이 심할 경우 편향된 트리가 생성될 수 있어 주의가 필요하다.

---

📚 Reference

- 박해선, 『혼자 공부하는 머신러닝 + 딥러닝』, 한빛미디어(2020)
- 권철민, 『파이썬 머신러닝 완벽 가이드』, 위키북스(2019)
- [사이킷런 공식문서](https://scikit-learn.org/stable/modules/tree.html)
- [티스토리, "머신러닝-12.편향(Bias)과 분산(Variance) Trade-off"](https://bkshin.tistory.com/entry/%EB%A8%B8%EC%8B%A0%EB%9F%AC%EB%8B%9D-12-%ED%8E%B8%ED%96%A5Bias%EC%99%80-%EB%B6%84%EC%82%B0Variance-Trade-off)