---
title : "ML (3) - 앙상블(Ensemble)"
description : "앙상블(Ensemble)에 대해 알아보자."
date : 2025-12-23
categories : [Development, ML&DL]
tags : [Scikit-learn, Ensemble]
pin : false
math : true
mermaid : true
---

# 부스팅(Boosting)

$$ \text{"약한 학습기로 강한 학습기를 만들수는 없을까?"} $$

**<mark>부스팅(Boosting)</mark>**은 이러한 관점에서 탄생했다. 

이전 포스트에서 결정트리에 대해 알아보았는데, 부스팅에서 대표적인 약한 분류기(weak learner)가 바로 **깊이가 얕은 결정트리**이다.

배깅과 같이 모델을 **병렬(parallel)**로 학습하는 것이 아닌, **순차적으로(sequentially)** 학습한다.

순차적으로 학습한다는 것은 이전 모델이 학습한 결과를 기반으로 다음 모델이 학습에 영향을 받는다는 것이다.

여러가지 부스팅 알고리즘이 존재하는데, 이를 통해 학습하는 방식을 알아보자.

# GBM(Gradient Boosting Machine)

**<mark>GBM</mark>**은 나중에 배우게 될 **경사하강법**을 핵심 원리로 사용하는 알고리즘이다.

여기에서 간단하게 설명하자면, 우리가 '손실 함수'라고 부르는 **산의 가장 낮은 곳**을 찾아 내려오는 과정이라고 할 수 있겠다.

```mermaid

flowchart TD
    %% 스타일 정의
    classDef data fill:#f9f,stroke:#333,stroke-width:2px,color:black;
    classDef model fill:#bbf,stroke:#333,stroke-width:2px,color:black;
    classDef calc fill:#ff9,stroke:#333,stroke-width:2px,stroke-dasharray: 5 5,color:black;
    classDef final fill:#bfb,stroke:#333,stroke-width:4px,color:black;

    %% 1단계: 초기화
    subgraph Step0 [Step 0: 초기 모델]
        Input([학습 데이터 <br/> 정답: y])
        InitModel[초기 예측값 <br/> 평균 등으로 설정]
        Input --> InitModel
    end

    %% 2단계: 첫 번째 반복
    subgraph Step1 [Step 1: 첫 번째 수정]
        Res1(잔차 1 계산 <br/> 오차 = 정답 - 초기예측)
        Tree1[트리 1 학습 <br/> 목표: 잔차 1 예측]
        Update1(예측값 업데이트 <br/> 초기값 + 학습률 × 트리1)
        
        InitModel --> Res1
        Input -.-> Res1
        Res1 --> Tree1
        Tree1 --> Update1
        InitModel -.-> Update1
    end

    %% 3단계: 두 번째 반복
    subgraph Step2 [Step 2: 두 번째 수정]
        Res2(잔차 2 계산 <br/> 오차 = 정답 - 업데이트1)
        Tree2[트리 2 학습 <br/> 목표: 잔차 2 예측]
        Update2(예측값 업데이트 <br/> 업데이트1 + 학습률 × 트리2)

        Update1 --> Res2
        Input -.-> Res2
        Res2 --> Tree2
        Tree2 --> Update2
        Update1 -.-> Update2
    end

    %% 최종 단계
    Final([최종 모델 <br/> 초기값 + α트리1 + α트리2 + ...])
    Update2 ==> Final

    %% 클래스 적용
    class Input data;
    class Tree1,Tree2 model;
    class Res1,Res2,Update1,Update2 calc;
    class Final final;
    class InitModel model;

```
첫 번째 트리는 원래 정답을 맞추는 게 아닌, 앞선 모델이 틀린 **오차(잔차, Residual)**를 학습 목표로 삼는다. 

그리고 결과가 생성되면 다음 트리로 넘어가는 **순차적인 구조**를 보인다.

학습 업데이트 과정에서 GBM은 학습률(`learning_rate`)과 트리의 개수(`n_estimators`)를 점진적으로 증가시키는 방식으로 성능향상을 기대한다.

또한 `Subsample`을 통해 각 트리의 학습에 쓰이는 훈련세트의 비율을 조정할 수 있는데, 역시 경사하강법에 관련이 깊은 내용이니 나중에 더 자세히 알아보겠다.

최종적으로 도출된 모델은 초기 예측값에 수많은 작은 나무들의 예측값을 조금씩 더한 형태가 되기 때문에 GBM이 높은 성능을 보여준다.

---
# HistGradientBoosting

앞서 살펴본 GBM은 순차적인 학습 방식으로 인해 **훈련 속도가 느리다**는 단점이 있다. (`n_jobs` 파라미터가 없다)

GBM의 속도와 성능을 더욱 강화한 방식이 바로 **<mark>히스토그램 기반 그레디언트 부스팅(HistGradientBoosting)</mark>**이다.

히스토그램 기반 그레디언트 부스팅의 핵심은 **입력 데이터를 256개의 구간(Bin)으로 노드를 분할**해 최적의 분할을 매우 빠르게 찾을 수 있다는 것이다.

```mermaid
flowchart TD
    %% 스타일 정의
    classDef raw fill:#e1f5fe,stroke:#01579b,stroke-width:2px,color:black;
    classDef bin fill:#fff9c4,stroke:#fbc02d,stroke-width:2px,color:black;
    classDef hist fill:#ffe0b2,stroke:#f57c00,stroke-width:2px,color:black;
    classDef split fill:#c8e6c9,stroke:#2e7d32,stroke-width:2px,color:black;
    classDef optim fill:#e1bee7,stroke:#4a148c,stroke-width:2px,stroke-dasharray: 5 5,color:black;

    %% 1단계: 전처리
    subgraph Preprocessing ["Step 1: 전처리 (Binning)"]
        RawData["원본 연속형 데이터 <br/> 0.1, 85.3, 12.4 ..."]:::raw
        Binning[["Binning 수행 <br/> 값의 범위를 256개 구간으로 나눔"]]:::bin
        BinnedData["정수형 데이터 변환 <br/> Bin ID: 0, 150, 23 ..."]:::raw
        
        RawData --> Binning --> BinnedData
    end

    %% 2단계: 히스토그램 생성
    subgraph Training ["Step 2: 학습 및 분기"]
        CalcGrad("잔차/Gradient 계산"):::raw
        
        BuildHist["히스토그램 생성 <br/> 데이터 하나하나가 아닌 <br/> 구간별 Gradient 합계 계산"]:::hist
        
        BinnedData --> CalcGrad --> BuildHist
        
        FindSplit{"최적 분기점 탐색 <br/> 256개 구간만 확인하면 됨! <br/> (속도 매우 빠름)"}:::split
        
        BuildHist --> FindSplit
    end
    
    %% 3단계: 최적화 기법
    subgraph Trick ["Step 3: 속도 최적화 (Subtraction Trick)"]
        ParentHist["부모 노드 히스토그램"]:::hist
        LeftHist["왼쪽 자식 히스토그램 <br/> 직접 계산"]:::hist
        RightHist["오른쪽 자식 히스토그램 <br/> 부모 - 왼쪽 = 오른쪽 <br/> (계산 생략 가능!)"]:::optim
        
        ParentHist --> LeftHist
        ParentHist -. "빼기 연산" .-> RightHist
    end

    FindSplit --> ParentHist
```

어떤 특성이 중요한지 확인 할 수 있는 특성 중요도를 간단하게 알아보자면 히스토그램이라는 통계학 관점에서 부스팅을 사용하므로

`permutation_importance()` 함수를 통해 반복하여 얻은 특성 중요도(`importance`), 평균(`importance_mean`), 표준편차(`importance_std`)를 확인할 수 있다.

---

📚 Reference

- 박해선, 『혼자 공부하는 머신러닝 + 딥러닝』, 한빛미디어(2020)
- 권철민, 『파이썬 머신러닝 완벽 가이드』, 위키북스(2019)
- [사이킷런 공식문서](https://scikit-learn.org/stable/modules/tree.html)
